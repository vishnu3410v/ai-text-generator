{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92c7e8c-9a35-45de-93c2-df927dfa9ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet openai huggingface_hub pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d798819c-63af-48df-a6a0-17c7f93b98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai import AuthenticationError, RateLimitError, APIConnectionError, BadRequestError\n",
    "\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# -----------------------------\n",
    "#  Choose your provider here:\n",
    "# -----------------------------\n",
    "# \"openai\"  \n",
    "# \"hf\"      \n",
    "PROVIDER = \"openai\"   # default to OpenAI;\n",
    "\n",
    "# -----------------------------\n",
    "# Keys (leave placeholders)\n",
    "# -----------------------------\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY\")\n",
    "os.environ.setdefault(\"HF_TOKEN\", \"YOUR_HF_TOKEN\")\n",
    "\n",
    "# -----------------------------\n",
    "# Model selection\n",
    "# -----------------------------\n",
    "\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"  \n",
    "\n",
    "# Hugging Face Models: gpt2, \n",
    "HF_MODEL = \"gpt2\"   # safe public default; can be swapped later\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1126ff4-b03d-413e-94cf-346c85261b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client ready.\n"
     ]
    }
   ],
   "source": [
    "openai_client = None\n",
    "hf_client = None\n",
    "\n",
    "if PROVIDER == \"openai\":\n",
    "    try:\n",
    "        openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        \n",
    "        print(\"OpenAI client ready.\")\n",
    "    except Exception as e:\n",
    "        print(\"OpenAI init failed. Falling back to Hugging Face.\\n\", repr(e))\n",
    "        PROVIDER = \"hf\"\n",
    "\n",
    "if PROVIDER == \"hf\":\n",
    "    try:\n",
    "        tok = os.getenv(\"HF_TOKEN\") \n",
    "        hf_client = InferenceClient(HF_MODEL, token=tok) if tok else InferenceClient(HF_MODEL)\n",
    "        print(f\"Hugging Face client ready ({HF_MODEL}).\")\n",
    "    except Exception as e:\n",
    "        print(\"HF init failed. Please check HF model name/token.\\n\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba5e208-9997-4580-b403-e182d9fa62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    prompt: str,\n",
    "    provider: str = PROVIDER,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 1.0,\n",
    "    max_tokens: int = 120,\n",
    "    frequency_penalty: float = 0.0,\n",
    "    presence_penalty: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns model text for a given prompt using either OpenAI or Hugging Face.\n",
    "    Handles common parameters and gracefully reports quota/auth issues.\n",
    "    \"\"\"\n",
    "    if provider == \"openai\":\n",
    "        if openai_client is None:\n",
    "            return \"[OpenAI not initialized. Provide a valid OPENAI_API_KEY or switch PROVIDER='hf']\"\n",
    "        try:\n",
    "            resp = openai_client.chat.completions.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Keep answers concise.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                max_tokens=max_tokens,\n",
    "                frequency_penalty=frequency_penalty,\n",
    "                presence_penalty=presence_penalty,\n",
    "            )\n",
    "            return resp.choices[0].message.content.strip()\n",
    "        except AuthenticationError:\n",
    "            return \"[OpenAI auth error: set OPENAI_API_KEY or ask your lead to add theirs]\"\n",
    "        except RateLimitError:\n",
    "            return \"[OpenAI rate/quota error: insufficient credits or rate limit hit]\"\n",
    "        except (APIConnectionError, BadRequestError) as e:\n",
    "            return f\"[OpenAI API error: {e}]\"\n",
    "        except Exception as e:\n",
    "            return f\"[OpenAI unexpected error: {repr(e)}]\"\n",
    "\n",
    "    # Hugging Face path (free)\n",
    "    if hf_client is None:\n",
    "        return \"[HF not initialized. Set PROVIDER='hf' or install huggingface_hub]\"\n",
    "    try:\n",
    "        # HF uses max_new_tokens; map from max_tokens\n",
    "        out = hf_client.text_generation(\n",
    "            prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            # return_full_text=False  # uncomment for some models if you only want the new text\n",
    "        )\n",
    "        # text_generation may return string or dict depending on backend; normalize\n",
    "        return out if isinstance(out, str) else str(out)\n",
    "    except Exception as e:\n",
    "        return f\"[HF error: {repr(e)}]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a863011b-5929-44c6-8724-34539ab3ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Write a short (4-5 sentences) explainer on why 'temperature' changes an LLM's writing style.\"\n",
    "\n",
    "baseline = generate_text(PROMPT, temperature=0.7, top_p=1.0, max_tokens=120)\n",
    "print(\"=== Baseline ===\\n\", baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3fe90-4af8-426b-96fd-670591d77223",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [0.0, 0.3, 0.7, 1.0]\n",
    "rows = []\n",
    "for t in temps:\n",
    "    text = generate_text(PROMPT, temperature=t, top_p=1.0, max_tokens=120)\n",
    "    rows.append({\"Experiment\": f\"temperature={t}\", \"Output\": text, \"Length(chars)\": len(text)})\n",
    "\n",
    "df_temp = pd.DataFrame(rows)\n",
    "df_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "089e0526-f8a4-427e-9e60-88844710a348",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROMPT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m tops:\n\u001b[1;32m----> 4\u001b[0m     text \u001b[38;5;241m=\u001b[39m generate_text(\u001b[43mPROMPT\u001b[49m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, top_p\u001b[38;5;241m=\u001b[39mp, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m)\n\u001b[0;32m      5\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength(chars)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(text)})\n\u001b[0;32m      7\u001b[0m df_topp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PROMPT' is not defined"
     ]
    }
   ],
   "source": [
    "tops = [0.3, 0.7, 1.0]\n",
    "rows = []\n",
    "for p in tops:\n",
    "    text = generate_text(PROMPT, temperature=0.7, top_p=p, max_tokens=120)\n",
    "    rows.append({\"Experiment\": f\"top_p={p}\", \"Output\": text, \"Length(chars)\": len(text)})\n",
    "\n",
    "df_topp = pd.DataFrame(rows)\n",
    "df_topp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4de48d38-6877-4b80-b77d-e49645f8a859",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROMPT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m lengths:\n\u001b[1;32m----> 4\u001b[0m     text \u001b[38;5;241m=\u001b[39m generate_text(\u001b[43mPROMPT\u001b[49m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39mm)\n\u001b[0;32m      5\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength(chars)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(text)})\n\u001b[0;32m      7\u001b[0m df_maxtok \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PROMPT' is not defined"
     ]
    }
   ],
   "source": [
    "lengths = [50, 100, 150]\n",
    "rows = []\n",
    "for m in lengths:\n",
    "    text = generate_text(PROMPT, temperature=0.7, top_p=0.95, max_tokens=m)\n",
    "    rows.append({\"Experiment\": f\"max_tokens={m}\", \"Output\": text, \"Length(chars)\": len(text)})\n",
    "\n",
    "df_maxtok = pd.DataFrame(rows)\n",
    "df_maxtok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4943649c-be4c-4f02-8f1b-7b3ace742091",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROMPT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PROVIDER \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m         text \u001b[38;5;241m=\u001b[39m generate_text(\u001b[43mPROMPT\u001b[49m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m, frequency_penalty\u001b[38;5;241m=\u001b[39mfp)\n\u001b[0;32m      5\u001b[0m         rows\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength(chars)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(text)})\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pp \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PROMPT' is not defined"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "if PROVIDER == \"openai\":\n",
    "    for fp in [0.0, 0.5, 1.0]:\n",
    "        text = generate_text(PROMPT, temperature=0.7, top_p=0.95, max_tokens=120, frequency_penalty=fp)\n",
    "        rows.append({\"Experiment\": f\"frequency_penalty={fp}\", \"Output\": text, \"Length(chars)\": len(text)})\n",
    "\n",
    "    for pp in [0.0, 0.5, 1.0]:\n",
    "        text = generate_text(PROMPT, temperature=0.7, top_p=0.95, max_tokens=120, presence_penalty=pp)\n",
    "        rows.append({\"Experiment\": f\"presence_penalty={pp}\", \"Output\": text, \"Length(chars)\": len(text)})\n",
    "\n",
    "    df_penalties = pd.DataFrame(rows)\n",
    "else:\n",
    "    df_penalties = pd.DataFrame([{\"Experiment\": \"penalties\", \"Output\": \"[Penalties not available on HF path]\", \"Length(chars)\": 0}])\n",
    "\n",
    "df_penalties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e146ba-b151-45c7-850e-63291a30ce30",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Combine available experiment tables\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m frames \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdf_temp\u001b[49m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop-p\u001b[39m\u001b[38;5;124m\"\u001b[39m, df_topp), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax Tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, df_maxtok)]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PROVIDER \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      4\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPenalties\u001b[39m\u001b[38;5;124m\"\u001b[39m, df_penalties))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_temp' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine available experiment tables\n",
    "frames = [(\"Temperature\", df_temp), (\"Top-p\", df_topp), (\"Max Tokens\", df_maxtok)]\n",
    "if PROVIDER == \"openai\":\n",
    "    frames.append((\"Penalties\", df_penalties))\n",
    "\n",
    "# Plot one chart per group showing output length (as a rough proxy for verbosity)\n",
    "for title, frame in frames:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(range(len(frame)), frame[\"Length(chars)\"])\n",
    "    plt.xticks(range(len(frame)), frame[\"Experiment\"], rotation=30, ha=\"right\")\n",
    "    plt.ylabel(\"Output length (chars)\")\n",
    "    plt.title(f\"{title} Sweep: Output Length\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
